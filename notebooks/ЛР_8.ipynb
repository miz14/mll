{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rFIgOM6i3Al"
   },
   "source": [
    "# Лабораторная работа 8. Обработка естественого языка. Начало"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDtmLvThDHOv"
   },
   "source": [
    "Обработка естественного языка (NLP) – это технология машинного обучения, которая дает компьютерам возможность интерпретировать, манипулировать и понимать человеческий язык. Сегодня организации имеют большие объемы голосовых и текстовых данных из различных каналов связи, таких как электронные письма, текстовые сообщения, новостные ленты социальных сетей, видео, аудио и многое другое. Они используют программное обеспечение NLP для автоматической обработки этих данных, анализа намерений или настроений в сообщении и реагирования на человеческое общение в режиме реального времени.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZULRBeHwDPK6"
   },
   "source": [
    "Обработка естественного языка сочетает в себе компьютерную лингвистику, машинное обучение и модели глубокого обучения для обработки человеческого языка.\n",
    "\n",
    "**Компьютерная лингвистика**\n",
    "Компьютерная лингвистика – это наука о понимании и построении моделей человеческого языка с помощью компьютеров и программных инструментов. Исследователи используют методы компьютерной лингвистики, такие как синтаксический и семантический анализ, для создания платформ, помогающих машинам понимать разговорный человеческий язык. Такие инструменты, как переводчики языков, синтезаторы текста в речь и программное обеспечение для распознавания речи, основаны на компьютерной лингвистике.\n",
    "\n",
    "**Машинное обучение**\n",
    "Машинное обучение – это технология, которая обучает компьютер с помощью выборочных данных для повышения его эффективности. Человеческий язык имеет несколько особенностей, таких как сарказм, метафоры, вариации в структуре предложений, а также исключения из грамматики и употребления, на изучение которых у людей уходят годы. Программисты используют методы машинного обучения, чтобы научить приложения NLP распознавать и точно понимать эти функции с самого начала.\n",
    "\n",
    "**Глубокое обучение**\n",
    "Глубокое обучение – это особая область машинного обучения, которая учит компьютеры учиться и мыслить как люди. Это включает нейросеть, состоящую из узлов обработки данных, напоминающих операции человеческого мозга. С помощью глубокого обучения компьютеры распознают, классифицируют и сопоставляют сложные закономерности во входных данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeR8-yebDVdY"
   },
   "source": [
    "## Этапы обработки\n",
    "В NLP используются методы предварительной обработки, такие как:\n",
    "1. токенизация,\n",
    "2. стемминг, лемматизация\n",
    "3. удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mcWYv6LJU9m",
    "outputId": "aa23afc3-52fd-4bfd-f772-a2e995f0463a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.2 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------------------- 0.1/12.2 MB 1.1 MB/s eta 0:00:11\n",
      "      --------------------------------------- 0.2/12.2 MB 1.4 MB/s eta 0:00:09\n",
      "      --------------------------------------- 0.3/12.2 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.4/12.2 MB 1.6 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.6/12.2 MB 2.3 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.2 MB 2.9 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.2 MB 3.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/12.2 MB 3.3 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.2 MB 5.1 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.3/12.2 MB 5.0 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.2/12.2 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.0/12.2 MB 6.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.3/12.2 MB 6.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.7/12.2 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.1/12.2 MB 6.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.4/12.2 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.8/12.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.2/12.2 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.6/12.2 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.0/12.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.4/12.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.8/12.2 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.2/12.2 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.6/12.2 MB 7.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.0/12.2 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.3/12.2 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.7/12.2 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.1/12.2 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.5/12.2 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.0/12.2 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.4/12.2 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 11.8/12.2 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.2/12.2 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.2/12.2 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.2/12.2 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in f:\\venv1\\venv\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 0.0/94.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 94.7/94.7 kB ? eta 0:00:00\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-win_amd64.whl (480 kB)\n",
      "     ---------------------------------------- 0.0/480.9 kB ? eta -:--:--\n",
      "     ------------------------------------  471.0/480.9 kB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- 480.9/480.9 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.4/2.1 MB 12.9 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.8/2.1 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.6/2.1 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.1/2.1 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------------- -------------------------- 0.5/1.5 MB 10.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: jinja2 in f:\\venv1\\venv\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy) (1.24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in f:\\venv1\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.5/7.0 MB 10.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.9/7.0 MB 9.9 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.5/7.0 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 2.0/7.0 MB 9.7 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.4/7.0 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 2.8/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.2/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.7/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.1/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 4.5/7.0 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 4.9/7.0 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 5.4/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 5.8/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.2/7.0 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 6.7/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.0/7.0 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.0/7.0 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in f:\\venv1\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\venv1\\venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\venv1\\venv\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.9 smart-open-6.3.0 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 tqdm-4.65.0 typer-0.7.0 wasabi-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-lookups-data\n",
      "  Downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 MB)\n",
      "     ---------------------------------------- 0.0/98.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/98.5 MB 1.4 MB/s eta 0:01:13\n",
      "     ---------------------------------------- 0.1/98.5 MB 1.2 MB/s eta 0:01:24\n",
      "     ---------------------------------------- 0.2/98.5 MB 1.2 MB/s eta 0:01:20\n",
      "     ---------------------------------------- 0.3/98.5 MB 1.4 MB/s eta 0:01:12\n",
      "     ---------------------------------------- 0.4/98.5 MB 1.6 MB/s eta 0:01:00\n",
      "     ---------------------------------------- 0.6/98.5 MB 2.0 MB/s eta 0:00:50\n",
      "     ---------------------------------------- 0.8/98.5 MB 2.5 MB/s eta 0:00:39\n",
      "     ---------------------------------------- 1.0/98.5 MB 3.0 MB/s eta 0:00:33\n",
      "     ---------------------------------------- 1.0/98.5 MB 3.0 MB/s eta 0:00:33\n",
      "      --------------------------------------- 1.7/98.5 MB 3.6 MB/s eta 0:00:27\n",
      "      --------------------------------------- 2.4/98.5 MB 4.9 MB/s eta 0:00:20\n",
      "     - -------------------------------------- 2.5/98.5 MB 4.4 MB/s eta 0:00:22\n",
      "     - -------------------------------------- 3.4/98.5 MB 5.5 MB/s eta 0:00:18\n",
      "     - -------------------------------------- 3.7/98.5 MB 5.6 MB/s eta 0:00:17\n",
      "     - -------------------------------------- 4.1/98.5 MB 5.9 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 4.5/98.5 MB 6.0 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 4.8/98.5 MB 6.0 MB/s eta 0:00:16\n",
      "     -- ------------------------------------- 5.2/98.5 MB 6.2 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 5.6/98.5 MB 6.2 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 5.9/98.5 MB 6.3 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 6.3/98.5 MB 6.4 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 6.6/98.5 MB 6.5 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 7.0/98.5 MB 6.5 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 7.3/98.5 MB 6.5 MB/s eta 0:00:15\n",
      "     --- ------------------------------------ 7.7/98.5 MB 6.6 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 8.1/98.5 MB 6.6 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 8.4/98.5 MB 6.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 8.8/98.5 MB 6.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 9.2/98.5 MB 6.9 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 9.6/98.5 MB 6.9 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 10.0/98.5 MB 6.9 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 10.4/98.5 MB 7.5 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 10.8/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 11.1/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 11.5/98.5 MB 8.5 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 11.9/98.5 MB 8.5 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 12.3/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 12.7/98.5 MB 8.5 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 13.1/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 13.4/98.5 MB 8.0 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 13.8/98.5 MB 8.0 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 14.3/98.5 MB 8.1 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 14.6/98.5 MB 8.1 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 15.0/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 15.4/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 15.9/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 16.2/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 16.6/98.5 MB 8.2 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 17.0/98.5 MB 8.3 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 17.4/98.5 MB 8.4 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 17.8/98.5 MB 8.4 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 18.2/98.5 MB 8.4 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 18.7/98.5 MB 8.4 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 19.1/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 19.5/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 19.9/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 20.3/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 20.7/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 21.1/98.5 MB 8.5 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 21.5/98.5 MB 8.6 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 21.9/98.5 MB 8.6 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 22.4/98.5 MB 8.6 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 22.7/98.5 MB 8.6 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 23.2/98.5 MB 8.6 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 23.6/98.5 MB 8.7 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.0/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.1/98.5 MB 8.8 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 24.6/98.5 MB 7.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 25.0/98.5 MB 7.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 25.4/98.5 MB 7.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 25.8/98.5 MB 7.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 26.2/98.5 MB 7.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 26.6/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ---------- ----------------------------- 27.0/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 27.5/98.5 MB 7.3 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 27.9/98.5 MB 7.3 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.3/98.5 MB 7.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 28.7/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ----------- ---------------------------- 29.1/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ----------- ---------------------------- 29.4/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 29.9/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 30.3/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 30.7/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 31.1/98.5 MB 6.1 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 31.6/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 32.0/98.5 MB 6.0 MB/s eta 0:00:12\n",
      "     ------------- -------------------------- 32.4/98.5 MB 6.0 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 32.8/98.5 MB 6.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 33.3/98.5 MB 6.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 33.7/98.5 MB 6.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 34.1/98.5 MB 6.1 MB/s eta 0:00:11\n",
      "     -------------- ------------------------- 34.5/98.5 MB 7.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 35.0/98.5 MB 7.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 35.4/98.5 MB 7.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 35.8/98.5 MB 7.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 36.2/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 36.7/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 37.1/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 37.5/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 38.0/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 38.4/98.5 MB 7.3 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 38.8/98.5 MB 9.1 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 39.2/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 39.6/98.5 MB 7.4 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 40.0/98.5 MB 7.4 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 40.5/98.5 MB 7.4 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 40.9/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 41.3/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 41.7/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 42.2/98.5 MB 7.6 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 42.6/98.5 MB 7.6 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 43.0/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 43.5/98.5 MB 7.6 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 43.9/98.5 MB 7.6 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 44.3/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 44.7/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 45.1/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 45.6/98.5 MB 7.5 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 46.0/98.5 MB 7.5 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 46.4/98.5 MB 7.5 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 46.8/98.5 MB 7.4 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 47.3/98.5 MB 7.5 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 47.7/98.5 MB 7.5 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 48.1/98.5 MB 7.4 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 48.5/98.5 MB 7.4 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 49.0/98.5 MB 7.5 MB/s eta 0:00:07\n",
      "     -------------------- ------------------- 49.4/98.5 MB 9.2 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 49.8/98.5 MB 9.1 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 50.2/98.5 MB 9.2 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 50.5/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 50.9/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 51.4/98.5 MB 9.1 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 51.8/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 52.2/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 52.7/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 53.1/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 53.5/98.5 MB 9.0 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 53.9/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 54.3/98.5 MB 8.8 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 54.7/98.5 MB 8.8 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 55.2/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 55.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 56.0/98.5 MB 8.8 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 56.4/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 56.6/98.5 MB 9.0 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 57.0/98.5 MB 7.1 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 57.4/98.5 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 57.8/98.5 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 58.4/98.5 MB 7.1 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 58.7/98.5 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 58.7/98.5 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 58.7/98.5 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 58.9/98.5 MB 6.5 MB/s eta 0:00:07\n",
      "     ------------------------ --------------- 59.3/98.5 MB 6.5 MB/s eta 0:00:07\n",
      "     ------------------------ --------------- 59.8/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 60.2/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 60.6/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 61.1/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 61.5/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 61.9/98.5 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 62.3/98.5 MB 6.6 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 62.7/98.5 MB 6.6 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 63.2/98.5 MB 6.6 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 63.6/98.5 MB 6.6 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 64.0/98.5 MB 6.7 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 64.4/98.5 MB 6.7 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 64.8/98.5 MB 6.6 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 65.3/98.5 MB 6.7 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 65.7/98.5 MB 6.7 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 66.1/98.5 MB 6.7 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 66.6/98.5 MB 6.7 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 67.0/98.5 MB 8.4 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 67.4/98.5 MB 8.4 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 67.8/98.5 MB 8.3 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 68.2/98.5 MB 8.3 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 68.7/98.5 MB 8.3 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 69.2/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 69.6/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.0/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.2/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.2/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.2/98.5 MB 9.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.4/98.5 MB 8.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 70.8/98.5 MB 8.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 71.3/98.5 MB 8.2 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 71.6/98.5 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 72.0/98.5 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 72.4/98.5 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 72.5/98.5 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 72.5/98.5 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 73.0/98.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 73.3/98.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 73.8/98.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 74.3/98.5 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 74.7/98.5 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 75.1/98.5 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 75.5/98.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 76.0/98.5 MB 7.5 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 76.4/98.5 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 76.8/98.5 MB 7.5 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 77.3/98.5 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 77.7/98.5 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 78.2/98.5 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 78.6/98.5 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 79.0/98.5 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 79.5/98.5 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 79.9/98.5 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 80.3/98.5 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 80.7/98.5 MB 8.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 80.7/98.5 MB 8.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 80.9/98.5 MB 8.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 81.3/98.5 MB 8.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 81.8/98.5 MB 8.2 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 82.2/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 82.7/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 83.0/98.5 MB 8.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 83.5/98.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 83.9/98.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 84.4/98.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 84.9/98.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 85.2/98.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 85.6/98.5 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 86.0/98.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 86.4/98.5 MB 8.5 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 86.8/98.5 MB 8.5 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 87.1/98.5 MB 8.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 87.6/98.5 MB 8.5 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 88.1/98.5 MB 8.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 88.1/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 88.6/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 89.0/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 89.4/98.5 MB 8.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 89.8/98.5 MB 8.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 90.4/98.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 90.8/98.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 91.2/98.5 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 91.7/98.5 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 92.2/98.5 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 92.7/98.5 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 93.1/98.5 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 93.5/98.5 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 94.0/98.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 94.4/98.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 94.6/98.5 MB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 95.0/98.5 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 95.0/98.5 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 95.7/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 96.0/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  96.3/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  96.7/98.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  97.0/98.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  97.3/98.5 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  97.6/98.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.1/98.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  98.4/98.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 98.5/98.5 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in f:\\venv1\\venv\\lib\\site-packages (from spacy-lookups-data) (63.2.0)\n",
      "Installing collected packages: spacy-lookups-data\n",
      "Successfully installed spacy-lookups-data-1.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
      "     --------------------------------------- 0.1/12.8 MB 919.0 kB/s eta 0:00:14\n",
      "     ---------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "      --------------------------------------- 0.2/12.8 MB 1.4 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.2/12.8 MB 4.9 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.2/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.1/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.7/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.0/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in f:\\venv1\\venv\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: setuptools in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in f:\\venv1\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in f:\\venv1\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\venv1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in f:\\venv1\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in f:\\venv1\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in f:\\venv1\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\venv1\\venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\venv1\\venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!pip install -U spacy-lookups-data\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHhgJ-tUJfx4"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErMWKmzhJV8j",
    "outputId": "47f806a5-128c-4585-bee5-c793a33e9291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: I\n",
      "Token 1: love\n",
      "Token 2: coding\n",
      "Token 3: and\n",
      "Token 4: writing\n"
     ]
    }
   ],
   "source": [
    "## tokenizing a piecen of text\n",
    "doc = \"I love coding and writing\"\n",
    "for i, w in enumerate(doc.split(\" \")):\n",
    "    print(\"Token \" + str(i) + \": \" + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LICJKScCQgcw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqtsSWbNQnXw",
    "outputId": "c93c199e-77b2-4039-dab1-53052e9827ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, m, student]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp('Im student')\n",
    "[token for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcXF43GnTCIs"
   },
   "source": [
    "## Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUpmCrvrS7G-",
    "outputId": "3620f483-bf77-44c9-943f-4b002890694a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stopwords = nlp.Defaults.stop_words\n",
    "type(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LBArOjUPT2AV"
   },
   "outputs": [],
   "source": [
    "my_stopwords.add('f*ck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63WNYQGvUH5b",
    "outputId": "915c3b93-da50-463c-a3a3-fa40e7caee75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'fun', ',']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# кто помнит Майка Чумакова, тот поймет\n",
    "doc = nlp('I Love fun, you')\n",
    "[token for token in doc if token.lower_ not in my_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2-Mv75YJhjI"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxqRXmzcJmnm",
    "outputId": "7a29b1a9-ffdc-49d9-b1dd-6415880881c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I => I\n",
      "love => love\n",
      "coding => code\n",
      ", => ,\n",
      "writing => write\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'code', ',', 'write']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import the libraries\n",
    "import spacy\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## lemmatization\n",
    "# токенизация (получение объекта doc)\n",
    "doc = nlp('I love coding, writing')\n",
    "# лемматизация\n",
    "for word in doc:\n",
    "    print(word.text, \"=>\", word.lemma_)\n",
    "data_lematized = [word.lemma_ for word in doc]\n",
    "data_lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 919.0 kB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 1.2 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.3/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.5/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.1/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.1/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.5/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in f:\\venv1\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.6.3-cp310-cp310-win_amd64.whl (268 kB)\n",
      "     ---------------------------------------- 0.0/268.0 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 194.6/268.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 268.0/268.0 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in f:\\venv1\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in f:\\venv1\\venv\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in f:\\venv1\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vpash/nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\share\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vpash\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vpash/nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\share\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vpash\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mF:\\venv1\\venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vpash/nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\share\\\\nltk_data'\n    - 'F:\\\\venv1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vpash\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iGjAE1qJu-O"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xMFI52qQkxz"
   },
   "source": [
    "# Word embedding -- векторное представление слов\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JxrYbiDRWXB"
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "Рассмотрим самый простой способ приведения текста к набору чисел. Для каждого слова посчитаем, как часто оно встречается в тексте. Результаты запишем в таблицу. Строки будут представлять тексты, столбцы -- слова. Если на пересечении строки с столбца стоит число 5, значит данное слово встретилось в данном тексте 5 раз. В большинстве ячеек будут нули. Поэтому хранить это всё удобнее в виде разреженных матриц (т.е. хранить только ненулевые значения).\n",
    "\n",
    "Таким образом, при построении \"мешка слов\" можно выделить следующие действия:\n",
    "\n",
    "1. Токенизация.\n",
    "\n",
    "2. Построение словаря: собираем все слова, которые встречались в текстах и пронумеровываем их (по алфавиту, например).\n",
    "\n",
    "3. Построение разреженной матрицы. В sklearn алгоритм приведения текста в bag-of-words реализован в виде класса CountVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeWNrMvmRVjG",
    "outputId": "3567dc4a-67a6-4bda-f0e6-2c94f53b783c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 48)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d111Us5hSxzL"
   },
   "source": [
    "Результат содержит 3 строки (для 3 текстов) и 48 столбцов (для 48 разных слов). Посмотрим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1io42iauSwTb",
    "outputId": "0165677e-3b8e-4736-82e8-9c7093ea924b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'великолепный': 5,\n",
       " 'сериал': 36,\n",
       " 'который': 12,\n",
       " 'поможет': 27,\n",
       " 'успокоить': 46,\n",
       " 'нервы': 20,\n",
       " 'при': 29,\n",
       " 'любых': 15,\n",
       " 'стрессах': 43,\n",
       " 'просто': 30,\n",
       " 'скрасит': 39,\n",
       " 'серые': 38,\n",
       " 'будни': 2,\n",
       " 'пожалуй': 25,\n",
       " 'если': 10,\n",
       " 'бы': 4,\n",
       " 'посмотрел': 28,\n",
       " 'только': 45,\n",
       " 'первые': 24,\n",
       " 'пару': 23,\n",
       " 'сезонов': 35,\n",
       " 'этого': 47,\n",
       " 'сериала': 37,\n",
       " 'легкой': 14,\n",
       " 'руки': 33,\n",
       " 'написал': 18,\n",
       " 'ему': 9,\n",
       " 'положительную': 26,\n",
       " 'рецензию': 32,\n",
       " 'общем': 22,\n",
       " 'создатели': 41,\n",
       " 'не': 19,\n",
       " 'вернут': 6,\n",
       " 'всё': 8,\n",
       " 'на': 17,\n",
       " 'круги': 13,\n",
       " 'своя': 34,\n",
       " 'то': 44,\n",
       " 'рейтинги': 31,\n",
       " 'следующих': 40,\n",
       " 'будут': 3,\n",
       " 'становится': 42,\n",
       " 'все': 7,\n",
       " 'ниже': 21,\n",
       " 'зрительская': 11,\n",
       " 'аудитория': 0,\n",
       " 'будет': 1,\n",
       " 'меньше': 16}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQVMFmKSTBfD",
    "outputId": "706ccbd9-2219-4696-f731-0faa3273c21a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0,\n",
       "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mECQhd7uTFqs"
   },
   "source": [
    "Как видим, ни стемминга, ни лемматизации по умолчанию не производится.\n",
    "Поэтому для уменьшения размерности данной матрицы требуется производить лемматизацию/стемминг и удаления стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEP3drzOTQgu"
   },
   "source": [
    "**Параметр min_df**\n",
    "\n",
    "Помимо выше озвученных есть и другие способы отсечения лишнего. Например, можно откидывать слова, которые встречаются слишком редко, с помощью параметра min_df. Установив min_df=2 мы откинем, все слова, которые встречаются менее, чем в 2 документах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUrVKE9eVvvK",
    "outputId": "152cbccc-d531-4ebd-bb37-60d48e689f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=2)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfA4vT56V_F5"
   },
   "source": [
    "**Биграммы, триграммы, n-граммы**\n",
    "\n",
    "По умолчанию bag-of-words (как следует из названия) представляет собой просто мешок слов. То есть для него предложения \"It's not good, it's bad!\" и \"It's not bad, it's good!\" абсолютно эквивалентны. Понятно, что при этом теряется много информации. Можно рассматривать не только отдельные слова, а последовательности длиной из 2 слов (биграммы), из 3 слов (триграммы) или в общем случае из n слов (n-граммы). На практике обычно задаётся диапазон от 1 до n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCpfC36oWBxB",
    "outputId": "1ff896b1-4430-46c3-ddf1-589759de6322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2, 'этого сериала': 4}\n",
      "[[1 1 0 0 0]]\n",
      "[[0 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "print(count_vectorizer.transform(['Если несколько сезонов']).todense())\n",
    "print(count_vectorizer.transform(['Этого сериала этого сезонов сезонов']).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riLA3FPAWRzP"
   },
   "source": [
    "**Ограничение количества признаков**\n",
    "\n",
    "Понятно, что с ростом n количество выделенных n-грамм быстро растёт. Для ограничения количества признаков можно использовать параметр max_features. В этом случае будет создано не более max_features признаков (будут выбраны самые часто встречающиеся слова и последовательности слов). Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64h04sKlWUH1",
    "outputId": "adeb72dd-59e3-4b34-9661-399c649e5a37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'поможет': 7,\n",
       " 'при': 11,\n",
       " 'просто': 13,\n",
       " 'поможет успокоить': 8,\n",
       " 'при любых': 12,\n",
       " 'просто скрасит': 14,\n",
       " 'если': 2,\n",
       " 'бы': 0,\n",
       " 'посмотрел': 9,\n",
       " 'сезонов': 21,\n",
       " 'этого': 23,\n",
       " 'сериала': 22,\n",
       " 'руки': 17,\n",
       " 'положительную': 5,\n",
       " 'рецензию': 16,\n",
       " 'посмотрел только': 10,\n",
       " 'этого сериала': 24,\n",
       " 'руки написал': 18,\n",
       " 'положительную рецензию': 6,\n",
       " 'своя': 19,\n",
       " 'все': 1,\n",
       " 'ниже': 4,\n",
       " 'меньше': 3,\n",
       " 'своя то': 20,\n",
       " 'рейтинги следующих': 15}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=25)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoGGIlJ1Wf5Z"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
    "\n",
    "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
    "\n",
    "$tfidf=tf∗idf$\n",
    "\n",
    "$idf=\\log\\frac{(N+1)}{(Nw+1)}+1$\n",
    "\n",
    "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
    "\n",
    "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
    "\n",
    "В sklearn есть класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioHGsp6AYkM3",
    "outputId": "36054a74-4db2-4c49-daa7-475e20493615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'великолепный': 5,\n",
       " 'сериал': 36,\n",
       " 'который': 12,\n",
       " 'поможет': 27,\n",
       " 'успокоить': 46,\n",
       " 'нервы': 20,\n",
       " 'при': 29,\n",
       " 'любых': 15,\n",
       " 'стрессах': 43,\n",
       " 'просто': 30,\n",
       " 'скрасит': 39,\n",
       " 'серые': 38,\n",
       " 'будни': 2,\n",
       " 'пожалуй': 25,\n",
       " 'если': 10,\n",
       " 'бы': 4,\n",
       " 'посмотрел': 28,\n",
       " 'только': 45,\n",
       " 'первые': 24,\n",
       " 'пару': 23,\n",
       " 'сезонов': 35,\n",
       " 'этого': 47,\n",
       " 'сериала': 37,\n",
       " 'легкой': 14,\n",
       " 'руки': 33,\n",
       " 'написал': 18,\n",
       " 'ему': 9,\n",
       " 'положительную': 26,\n",
       " 'рецензию': 32,\n",
       " 'общем': 22,\n",
       " 'создатели': 41,\n",
       " 'не': 19,\n",
       " 'вернут': 6,\n",
       " 'всё': 8,\n",
       " 'на': 17,\n",
       " 'круги': 13,\n",
       " 'своя': 34,\n",
       " 'то': 44,\n",
       " 'рейтинги': 31,\n",
       " 'следующих': 40,\n",
       " 'будут': 3,\n",
       " 'становится': 42,\n",
       " 'все': 7,\n",
       " 'ниже': 21,\n",
       " 'зрительская': 11,\n",
       " 'аудитория': 0,\n",
       " 'будет': 1,\n",
       " 'меньше': 16}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmKX6oGcYqSm"
   },
   "source": [
    "Словарь содержит те же 48 значений, которые были бы и для CountVectorizer. Но значения в таблице другие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cpgf0X77YmNn",
    "outputId": "b29299b0-f4be-4aa3-d497-d6652d529b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.2773501 , 0.        , 0.2773501 ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2773501 , 0.        , 0.2773501 , 0.2773501 ,\n",
       "         0.        , 0.        , 0.        , 0.2773501 , 0.        ,\n",
       "         0.        , 0.2773501 , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.48065817,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.24032909,\n",
       "         0.18277647, 0.        , 0.        , 0.        , 0.24032909,\n",
       "         0.        , 0.        , 0.        , 0.24032909, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.24032909, 0.24032909,\n",
       "         0.24032909, 0.24032909, 0.        , 0.24032909, 0.        ,\n",
       "         0.        , 0.        , 0.24032909, 0.24032909, 0.        ,\n",
       "         0.18277647, 0.        , 0.18277647, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.24032909, 0.        , 0.18277647],\n",
       "        [0.18162735, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
       "         0.        , 0.18162735, 0.36325471, 0.18162735, 0.        ,\n",
       "         0.13813228, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
       "         0.        , 0.36325471, 0.18162735, 0.        , 0.18162735,\n",
       "         0.        , 0.36325471, 0.18162735, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.18162735, 0.        , 0.        , 0.18162735,\n",
       "         0.13813228, 0.        , 0.13813228, 0.        , 0.        ,\n",
       "         0.18162735, 0.18162735, 0.18162735, 0.        , 0.18162735,\n",
       "         0.        , 0.        , 0.13813228]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mODMPXvhZBbo"
   },
   "source": [
    "Ненулевые значения находятся на тех же местах, но отмасштабированы в зависимости от частоты слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1k2kwuOZFj3"
   },
   "source": [
    "**Параметр sublinear_tf**\n",
    "\n",
    "Большая часть параметров у CountVectorizer и TfidfVectorizer одинакова. Но у TfidfVectorizer есть один важный дополнительный параметр.\n",
    "\n",
    "Как видно из формулы tfidf = tf * idf, если слово будет встречаться не один, а два раза, то tfidf вырастет в два раза. Если слово будет встречаться не один, а 10 раз, то tfidf вырастет почти в 10 раз. В качестве примера добавим в третью строку ещё пару слов меньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0F-HviBYvoA",
    "outputId": "a0366dc2-7c7b-4109-cc10-d8c799f08963"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.15373049, 0.15373049, 0.        , 0.15373049, 0.        ,\n",
       "         0.        , 0.15373049, 0.30746099, 0.15373049, 0.        ,\n",
       "         0.116916  , 0.15373049, 0.        , 0.15373049, 0.        ,\n",
       "         0.        , 0.61492198, 0.15373049, 0.        , 0.15373049,\n",
       "         0.        , 0.30746099, 0.15373049, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.15373049, 0.        , 0.        , 0.15373049,\n",
       "         0.116916  , 0.        , 0.116916  , 0.        , 0.        ,\n",
       "         0.15373049, 0.15373049, 0.15373049, 0.        , 0.15373049,\n",
       "         0.        , 0.        , 0.116916  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше и меньше и меньше.\"]\n",
    "TfidfVectorizer().fit_transform(texts).todense()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfDTxTa2ZSWH"
   },
   "source": [
    "Значение tfidf слова \"меньше\" выросло с 0.36325471 до 0.61492198, а остальные упали .\n",
    "\n",
    "Вопрос - хотим ли мы таких сильных изменений. Если не хотим, то можно использовать параметр sublinear_tf=True. При его использовании вместо tf будет браться 1 + log(tf). То есть по-прежнему с ростом tf будет расти и tfidf, но уже не так радикально (и соответственно остальные значения будут уменьшаться не так быстро). Для некоторых задач это может дать прирост в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBgHVX_pZKsY",
    "outputId": "78a4d24f-7770-4fb4-9e72-16c16f9d69bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.18336592, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
       "         0.        , 0.18336592, 0.31046549, 0.18336592, 0.        ,\n",
       "         0.13945451, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
       "         0.        , 0.43756505, 0.18336592, 0.        , 0.18336592,\n",
       "         0.        , 0.31046549, 0.18336592, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.18336592, 0.        , 0.        , 0.18336592,\n",
       "         0.13945451, 0.        , 0.13945451, 0.        , 0.        ,\n",
       "         0.18336592, 0.18336592, 0.18336592, 0.        , 0.18336592,\n",
       "         0.        , 0.        , 0.13945451]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer(sublinear_tf=True).fit_transform(texts).todense()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLI-fF73bUdn",
    "outputId": "1920f2e2-23a1-4e71-fb83-cdd3969b306a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGAL_G_zDupn"
   },
   "source": [
    "# LDA\n",
    "\n",
    "LDA принадлежит семейству порождающий вероятностных моделей, в которых темы представлены вероятностями появления каждого слова из заданного набора. Документы в свою очередь могут быть представлены как сочетания тем. Уникальная особенность моделей LDA состоит в том что темы не обязательно должны быть различными и слова могут встречаться в нескольких темах; это придает некоторую нечеткость определяемым темам, что может пригодиться для совладения с гибкостью языка.\n",
    "\n",
    "Для проведения тематического моделирования с помощью LDA можно использовать [sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "x4uSDKYwEwx3"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yIH6LoX6E40P"
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_o-owByVH5zB"
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0BJQUwMH6Z5",
    "outputId": "b5b32adc-6771-4e2f-f0df-5dedda4cea58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4jL4yebIILz",
    "outputId": "4f5d5840-1795-487a-d12f-e58c3cfea426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqn03YEof36y",
    "outputId": "a9ed1935-c456-46aa-ce48-d11942d2f227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xQftgYw8IAvw"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdjMq9E9IS6K",
    "outputId": "38830090-61a6-4111-f3ea-f7fbf921e5a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '10', '100', '11', '12', '128', '13', '130', '14',\n",
       "       '15', '16', '17', '18', '19', '1992', '1993', '20', '200', '21',\n",
       "       '22', '23', '24', '25', '250', '26', '27', '28', '29', '2nd', '30',\n",
       "       '300', '31', '32', '33', '34', '35', '36', '37', '38', '3d', '40',\n",
       "       '42', '43', '44', '45', '48', '49', '50', '500', '51', '55', '60',\n",
       "       '66', '70', '72', '75', '80', '800', '86', '90', '92', '93', '__',\n",
       "       'able', 'ac', 'accept', 'access', 'according', 'act', 'action',\n",
       "       'actually', 'add', 'added', 'addition', 'address',\n",
       "       'administration', 'advance', 'age', 'ago', 'agree', 'aids', 'air',\n",
       "       'al', 'allow', 'allowed', 'alt', 'america', 'american', 'amiga',\n",
       "       'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody',\n",
       "       'apartment', 'appears', 'apple', 'application', 'applications',\n",
       "       'apply', 'appreciated', 'approach', 'appropriate', 'apr', 'april',\n",
       "       'archive', 'area', 'areas', 'aren', 'argument', 'armenia',\n",
       "       'armenian', 'armenians', 'army', 'article', 'ask', 'asked',\n",
       "       'asking', 'assume', 'atheism', 'attack', 'attacks', 'attempt',\n",
       "       'au', 'author', 'authority', 'available', 'average', 'away',\n",
       "       'azerbaijan', 'bad', 'based', 'basic', 'basically', 'begin',\n",
       "       'belief', 'believe', 'best', 'better', 'bible', 'big', 'bike',\n",
       "       'billion', 'bios', 'bit', 'black', 'block', 'blood', 'blue',\n",
       "       'board', 'bob', 'body', 'book', 'books', 'bought', 'box', 'brake',\n",
       "       'break', 'bring', 'btw', 'build', 'building', 'built', 'bus',\n",
       "       'business', 'buy', 'buying', 'ca', 'cable', 'called', 'calls',\n",
       "       'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars',\n",
       "       'case', 'cases', 'cause', 'cc', 'cd', 'center', 'certain',\n",
       "       'certainly', 'chance', 'change', 'changed', 'changes', 'cheap',\n",
       "       'check', 'children', 'chip', 'choice', 'christ', 'christian',\n",
       "       'christians', 'church', 'citizens', 'city', 'claim', 'clear',\n",
       "       'clearly', 'clinton', 'clipper', 'clock', 'close', 'code', 'cold',\n",
       "       'color', 'com', 'come', 'comes', 'coming', 'command', 'comments',\n",
       "       'commercial', 'common', 'communications', 'community', 'comp',\n",
       "       'company', 'complete', 'completely', 'computer', 'condition',\n",
       "       'conference', 'congress', 'connector', 'consider', 'considered',\n",
       "       'contact', 'containing', 'contains', 'continue', 'control',\n",
       "       'controller', 'copies', 'copy', 'correct', 'cost', 'costs',\n",
       "       'couldn', 'countries', 'country', 'couple', 'course', 'court',\n",
       "       'cover', 'create', 'created', 'crime', 'crowd', 'cs', 'cubs',\n",
       "       'current', 'currently', 'cut', 'dangerous', 'data', 'database',\n",
       "       'date', 'dave', 'david', 'day', 'days', 'dc', 'dead', 'deal',\n",
       "       'death', 'dec', 'decided', 'defense', 'deleted', 'department',\n",
       "       'design', 'designed', 'details', 'developed', 'development',\n",
       "       'device', 'devices', 'did', 'didn', 'die', 'died', 'difference',\n",
       "       'different', 'difficult', 'digital', 'directly', 'directory',\n",
       "       'discussion', 'disease', 'disk', 'display', 'division', 'does',\n",
       "       'doesn', 'dog', 'doing', 'dollars', 'don', 'door', 'dos', 'dot',\n",
       "       'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'driving',\n",
       "       'drug', 'earlier', 'early', 'earth', 'easily', 'easy', 'edu',\n",
       "       'effect', 'effective', 'effort', 'email', 'encrypted',\n",
       "       'encryption', 'end', 'energy', 'enforcement', 'engine', 'entire',\n",
       "       'equipment', 'eric', 'error', 'especially', 'events', 'evidence',\n",
       "       'exactly', 'example', 'excellent', 'exist', 'expect', 'experience',\n",
       "       'explain', 'extra', 'face', 'fact', 'fair', 'fairly', 'faith',\n",
       "       'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax',\n",
       "       'feature', 'features', 'federal', 'feel', 'field', 'figure',\n",
       "       'file', 'files', 'final', 'finally', 'fine', 'firearm', 'fit',\n",
       "       'floppy', 'flyers', 'folks', 'follow', 'following', 'food',\n",
       "       'force', 'forget', 'form', 'format', 'formats', 'free', 'freedom',\n",
       "       'friend', 'ftp', 'function', 'future', 'game', 'games', 'gas',\n",
       "       'gave', 'gay', 'general', 'generally', 'gets', 'getting', 'given',\n",
       "       'gives', 'giving', 'gm', 'goal', 'god', 'goes', 'going', 'gone',\n",
       "       'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek',\n",
       "       'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half',\n",
       "       'hand', 'happen', 'happened', 'happens', 'happy', 'hard',\n",
       "       'hardware', 'haven', 'having', 'head', 'heads', 'health', 'hear',\n",
       "       'heard', 'heart', 'heaven', 'held', 'hell', 'help', 'hi', 'high',\n",
       "       'higher', 'history', 'hit', 'hiv', 'hockey', 'hold', 'home',\n",
       "       'hope', 'hospital', 'hot', 'hours', 'house', 'hp', 'human', 'ibm',\n",
       "       'id', 'idea', 'ii', 'image', 'images', 'imagine', 'important',\n",
       "       'include', 'included', 'includes', 'including', 'increase',\n",
       "       'individual', 'info', 'information', 'input', 'inside', 'instead',\n",
       "       'insurance', 'interested', 'interesting', 'interface', 'internal',\n",
       "       'international', 'internet', 'involved', 'isn', 'israel',\n",
       "       'israeli', 'issue', 'james', 'jesus', 'jewish', 'jews', 'job',\n",
       "       'jobs', 'john', 'just', 'kept', 'key', 'keys', 'kill', 'killed',\n",
       "       'killing', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows',\n",
       "       'lack', 'land', 'language', 'large', 'late', 'later', 'latest',\n",
       "       'launch', 'law', 'laws', 'leafs', 'learn', 'leave', 'led', 'left',\n",
       "       'legal', 'let', 'letter', 'level', 'library', 'license', 'life',\n",
       "       'light', 'like', 'likely', 'limited', 'line', 'lines', 'list',\n",
       "       'listen', 'little', 'live', 'lives', 'living', 'll', 'local',\n",
       "       'long', 'longer', 'look', 'looking', 'looks', 'lord', 'lost',\n",
       "       'lot', 'lots', 'love', 'low', 'luck', 'lunar', 'mac', 'machine',\n",
       "       'machines', 'magi', 'mail', 'main', 'major', 'make', 'makes',\n",
       "       'making', 'mamma', 'man', 'manager', 'manual', 'mark', 'market',\n",
       "       'marriage', 'mars', 'mary', 'mass', 'master', 'math', 'matter',\n",
       "       'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical',\n",
       "       'member', 'members', 'memory', 'men', 'mention', 'mentioned',\n",
       "       'message', 'middle', 'mike', 'mil', 'miles', 'military', 'million',\n",
       "       'mind', 'mission', 'mit', 'mode', 'model', 'models', 'modern',\n",
       "       'money', 'monitor', 'months', 'moon', 'moral', 'mother', 'motif',\n",
       "       'mr', 'ms', 'nasa', 'national', 'nature', 'navy', 'near',\n",
       "       'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new',\n",
       "       'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal',\n",
       "       'north', 'note', 'nsa', 'number', 'numbers', 'objects', 'obvious',\n",
       "       'offer', 'office', 'oh', 'oil', 'ok', 'old', 'older', 'ones',\n",
       "       'open', 'opinion', 'opinions', 'orbit', 'order', 'org',\n",
       "       'organization', 'original', 'os', 'output', 'outside', 'package',\n",
       "       'page', 'paper', 'papers', 'parents', 'particular', 'parts',\n",
       "       'party', 'pass', 'past', 'paul', 'pay', 'pc', 'people',\n",
       "       'performance', 'period', 'person', 'personal', 'peter', 'phone',\n",
       "       'pick', 'piece', 'pin', 'pittsburgh', 'place', 'places', 'plan',\n",
       "       'play', 'played', 'player', 'players', 'playing', 'plus', 'point',\n",
       "       'points', 'police', 'policy', 'political', 'population', 'port',\n",
       "       'position', 'possible', 'post', 'posted', 'posting', 'power', 'pp',\n",
       "       'present', 'president', 'press', 'pretty', 'price', 'printer',\n",
       "       'private', 'pro', 'probably', 'probe', 'probes', 'problem',\n",
       "       'problems', 'process', 'product', 'program', 'programs', 'project',\n",
       "       'prove', 'provide', 'pub', 'public', 'purpose', 'putting',\n",
       "       'quality', 'question', 'questions', 'quite', 'radio', 'ram',\n",
       "       'range', 'rate', 'rates', 'ray', 'read', 'reading', 'real',\n",
       "       'really', 'reason', 'reasonable', 'received', 'recent', 'recently',\n",
       "       'recommend', 'record', 'red', 'reference', 'regular', 'related',\n",
       "       'release', 'religion', 'religious', 'remember', 'reply', 'report',\n",
       "       'request', 'require', 'required', 'research', 'response', 'rest',\n",
       "       'result', 'results', 'return', 'right', 'rights', 'road', 'robert',\n",
       "       'role', 'rom', 'room', 'rules', 'run', 'running', 'runs', 'safety',\n",
       "       'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says',\n",
       "       'school', 'sci', 'science', 'scientific', 'screen', 'scsi',\n",
       "       'search', 'season', 'second', 'section', 'secure', 'security',\n",
       "       'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial',\n",
       "       'series', 'seriously', 'server', 'service', 'set', 'sex', 'sgi',\n",
       "       'shall', 'short', 'shot', 'shots', 'shows', 'shuttle', 'signal',\n",
       "       'similar', 'simple', 'simply', 'sin', 'single', 'site',\n",
       "       'situation', 'size', 'slow', 'small', 'society', 'software',\n",
       "       'solar', 'sold', 'soldiers', 'soon', 'sorry', 'sort', 'sound',\n",
       "       'sounds', 'source', 'sources', 'soviet', 'space', 'spacecraft',\n",
       "       'special', 'specific', 'specifically', 'speed', 'st', 'standard',\n",
       "       'start', 'started', 'starting', 'state', 'statement', 'states',\n",
       "       'station', 'stay', 'stop', 'story', 'street', 'strong', 'study',\n",
       "       'stuff', 'stupid', 'subject', 'suggest', 'sun', 'support',\n",
       "       'supported', 'supports', 'suppose', 'supposed', 'sure', 'surface',\n",
       "       'switch', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking',\n",
       "       'tape', 'team', 'teams', 'technical', 'technology', 'tell', 'term',\n",
       "       'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things',\n",
       "       'think', 'thinking', 'thought', 'time', 'times', 'tires', 'today',\n",
       "       'told', 'took', 'toronto', 'total', 'town', 'traffic', 'transfer',\n",
       "       'tried', 'trouble', 'true', 'trust', 'truth', 'try', 'trying',\n",
       "       'turkish', 'turn', 'turned', 'tv', 'type', 'types', 'uk',\n",
       "       'understand', 'unfortunately', 'united', 'university', 'unix',\n",
       "       'unless', 'use', 'used', 'useful', 'user', 'users', 'uses',\n",
       "       'using', 'usually', 'value', 'van', 'various', 've', 'version',\n",
       "       'vga', 'video', 'view', 'volume', 'vs', 'want', 'wanted', 'wants',\n",
       "       'war', 'washington', 'wasn', 'water', 'way', 'weapon', 'weapons',\n",
       "       'week', 'weeks', 'went', 'western', 'white', 'wife', 'willing',\n",
       "       'win', 'window', 'windows', 'wish', 'woman', 'women', 'won',\n",
       "       'wonder', 'wondering', 'word', 'words', 'work', 'worked',\n",
       "       'working', 'works', 'world', 'worse', 'worth', 'wouldn', 'write',\n",
       "       'written', 'wrong', 'xfree86', 'year', 'years', 'yes', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fTjSFEehCCa",
    "outputId": "ac8663e5-e1a7-432b-d497-3bcd51c72d74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица темы-слова\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5utkTYOBh_cQ",
    "outputId": "a2640a62-2eaa-4f79-d669-4aafee4f8d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица документы-темы\n",
    "lda.transform(tf).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtN1TO9ziLKI",
    "outputId": "8bc252e2-5a67-4b1c-f5ce-33aa1fcc98e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "edu com mail send graphics ftp pub available contact version\n",
      "Topic #1:\n",
      "don like just think know ve good way people right\n",
      "Topic #2:\n",
      "think christian book atheism new pittsburgh president like game just\n",
      "Topic #3:\n",
      "drive windows disk thanks use card drives hard using software\n",
      "Topic #4:\n",
      "hiv health aids april disease research care medical information 1993\n",
      "Topic #5:\n",
      "god people does jesus law say just life don israel\n",
      "Topic #6:\n",
      "10 55 11 15 game 12 18 team 20 19\n",
      "Topic #7:\n",
      "car year new bike cars good engine just price oil\n",
      "Topic #8:\n",
      "people said didn went did know time just like took\n",
      "Topic #9:\n",
      "key space government public use law encryption section keys earth\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWh582N5ZZnJ"
   },
   "source": [
    "# Задание\n",
    "Используя данные https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "1. Самостоятельно реализовать BoW, TF-IDF\n",
    "2. Решить задачу классификации с понижением размерности. Использовать самостоятельно реализованные модели из предыдущих ЛР.\n",
    "3. Решить задачу мягкой кластеризации (ТМ) с помощью LDA\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
